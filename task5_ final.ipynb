{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task5_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "6f6a9d06c0fd64bf60e6e1c5f52d9e61e3a0d126987fc458258eb07643fb7172"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Intro to TensorFlow: Classify images of clothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "source": [
        "# import Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "\n",
        "# import the other helper libraries required\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## Import the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLdCchMdCaWQ"
      },
      "source": [
        "This guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the articles of clothing you'll use here.\n",
        "\n",
        "This guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n",
        "\n",
        "Here, 60,000 images are used to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and [load the Fashion MNIST data](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data) directly from TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MqDQO0KCaWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda231c7-ca77-41f9-ad8f-114a94509cbe"
      },
      "source": [
        "#complete the code below:\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "#load the data into train_images, train_labels, test_images, test_labels\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9FDsUlxCaWW"
      },
      "source": [
        "Loading the dataset returns four NumPy arrays:\n",
        "\n",
        "* The `train_images` and `train_labels` arrays are the *training set*—the data the model uses to learn.\n",
        "* The model is tested against the *test set*, the `test_images`, and `test_labels` arrays.\n",
        "\n",
        "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjnLH5S2CaWx"
      },
      "source": [
        "#store the class names in a list\n",
        "#complete the code below\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brm0b_KACaWX"
      },
      "source": [
        "## Explore the data\n",
        "\n",
        "Let's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW5k_xz1CaWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f23b13-30d6-49eb-9f54-ca1e9d7a61ee"
      },
      "source": [
        "#check the dimensions of the training data\n",
        "train_images.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIAcvQqMCaWf"
      },
      "source": [
        "Likewise, there are 60,000 labels in the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRFYHB2mCaWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eafbf7e0-2c66-4742-f82c-c6388c5154d5"
      },
      "source": [
        "#verify the number of train_lables\n",
        "len(train_labels)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMPI88iZpO2T"
      },
      "source": [
        "There are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KFnYlcwCaWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d9d2de-e5e1-49e2-8f00-34921d165506"
      },
      "source": [
        "#verify the number of images in test_images by checking the dimensions\n",
        "print(test_images.shape)\n",
        "print(len(test_images))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 28, 28)\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd0A0Iu0CaWq"
      },
      "source": [
        "And the test set contains 10,000 images labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJmPr5-ACaWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92fb94de-06ef-4e0c-db67-6d7463309f12"
      },
      "source": [
        "#verify the number of images in test_labels\n",
        "len(test_labels)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES6uQoLKCaWr"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4VEw8Ud9Quh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5bd253e8-fe18-49ad-8edd-185aff9f9225"
      },
      "source": [
        "#plot a figure using the imshow() function to view the first image in train_images (train_images[0])\n",
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz7l27Lz9S1P"
      },
      "source": [
        "Scale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the *training set* and the *testing set* be preprocessed in the same way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW5WzIPlCaWv"
      },
      "source": [
        "#complete the code below\n",
        "train_images = train_images /255.0\n",
        "\n",
        "test_images = test_images /255.0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee638AlnCaWz"
      },
      "source": [
        "To verify that the data is in the correct format and that you're ready to build and train the network, display the first 25 images from the *training set* and display the class name below each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZTImqg_CaW1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "06314117-e15a-4f85-ce9c-a5c65e07965c"
      },
      "source": [
        "#write code below\n",
        "plt.figure(figsize=(15,15))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59veuiEZCaW4"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Building the neural network requires configuring the layers of the model, then compiling the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxg1XGm0eOBy"
      },
      "source": [
        "### Set up the layers\n",
        "\n",
        "The basic building block of a neural network is the [*layer*](https://www.tensorflow.org/api_docs/python/tf/keras/layers). Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.\n",
        "\n",
        "Most of deep learning consists of chaining together simple layers. Most layers, such as `tf.keras.layers.Dense`, have parameters that are learned during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ODch-OFCaW4"
      },
      "source": [
        "#create a model having the layers defined below this cell\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gut8A_7rCaW6"
      },
      "source": [
        "The first layer in this network, `tf.keras.layers.Flatten`, transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n",
        "\n",
        "After the pixels are flattened, the network consists of a sequence of two `tf.keras.layers.Dense` layers with a 'BatchNormalization()' layer in between the 2 dense layers. These are densely connected, or fully connected, neural layers. The first `Dense` layer has 128 nodes (or neurons). The second (and last) layer returns a logits array with length of 10. Each node contains a score that indicates the current image belongs to one of the 10 classes. \n",
        "The Batch Normalization layer works by performing a series of operations on the incoming input data. The set of operations involves standardization, normalization, rescaling and shifting of offset of input values coming into the BN layer.\n",
        "\n",
        "### Compile the model\n",
        "\n",
        "Before the model is ready for training, it needs a few more settings. These are added during the model's [*compile*](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) step:\n",
        "\n",
        "* [*Loss function*](https://www.tensorflow.org/api_docs/python/tf/keras/losses) —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
        "* [*Optimizer*](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) —This is how the model is updated based on the data it sees and its loss function.\n",
        "* [*Metrics*](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) —Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhan11blCaW7"
      },
      "source": [
        "#compile the model using model.compile using the adam optimizer, SparseCategoricalCrossentropy loss and accuracy as metrics\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKF6uW-BCaW-"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Training the neural network model requires the following steps:\n",
        "\n",
        "1. Feed the training data to the model. In this example, the training data is in the `train_images` and `train_labels` arrays.\n",
        "2. The model learns to associate images and labels.\n",
        "3. You ask the model to make predictions about a test set—in this example, the `test_images` array.\n",
        "4. Verify that the predictions match the labels from the `test_labels` array.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4P4zIV7E28Z"
      },
      "source": [
        "### Feed the model\n",
        "\n",
        "To start training,  call the [`model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) method—so called because it \"fits\" the model to the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvwvpA64CaW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d7862a-503a-4777-e9fa-0f77dbaee6a8"
      },
      "source": [
        "#write code below setting epochs=10:\n",
        "model_result = model.fit(train_images, train_labels,  epochs=10 )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5020 - accuracy: 0.8237\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3781 - accuracy: 0.8635\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3392 - accuracy: 0.8759\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3157 - accuracy: 0.8827\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2946 - accuracy: 0.8915\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2818 - accuracy: 0.8963\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2682 - accuracy: 0.9019\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2562 - accuracy: 0.9045\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2461 - accuracy: 0.9077\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2383 - accuracy: 0.9118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3ZVOhugCaXA"
      },
      "source": [
        "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.91 (or 91%) on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCpr6DGyE28h"
      },
      "source": [
        "### Evaluate accuracy\n",
        "\n",
        "Next, compare how the model performs on the test dataset using model.evaluate():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VflXLEeECaXC"
      },
      "source": [
        "#complete code below\n",
        "\n",
        "test_loss, test_acc = \n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWfgsmVXCaXG"
      },
      "source": [
        "It turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy represents *overfitting*. Overfitting happens when a machine learning model performs worse on new, previously unseen inputs than it does on the training data. An overfitted model \"memorizes\" the noise and details in the training dataset to a point where it negatively impacts the performance of the model on the new data. For more information, see the following:\n",
        "*   [Demonstrate overfitting](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit#demonstrate_overfitting)\n",
        "*   [Strategies to prevent overfitting](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit#strategies_to_prevent_overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-PyD1SYE28q"
      },
      "source": [
        "### Make predictions\n",
        "\n",
        "With the model trained, you can use it to make predictions about some images.\n",
        "The model's linear outputs, [logits](https://developers.google.com/machine-learning/glossary#logits). Attach a softmax layer to convert the logits to probabilities, which are easier to interpret. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnfNA0CrQLSD"
      },
      "source": [
        "#complete code below\n",
        "probability_model = model.predict(test_images)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl91RPhdCaXI"
      },
      "source": [
        "#predict on the test_images using the probability_model\n",
        "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
        "predictions = probability_model.predict(test_images)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Kk1voUCaXJ"
      },
      "source": [
        "Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DmJEUinCaXK"
      },
      "source": [
        "predictions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hw1hgeSCaXN"
      },
      "source": [
        "A prediction is an array of 10 numbers. They represent the model's \"confidence\" that the image corresponds to each of the 10 different articles of clothing. You can see which label has the highest confidence value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsqenuPnCaXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f4250b-90a8-4991-eaf0-b47f2d73569e"
      },
      "source": [
        "#use np.argmax to find the highest confidence value\n",
        "print(\"Highest Confidence Value is of class : \",np.argmax(predictions[0]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest Confidence Value is of class :  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E51yS7iCCaXO"
      },
      "source": [
        "So, the model is most confident that this image is an ankle boot, or `class_names[9]`. Examining the test label shows that this classification is correct:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd7Pgsu6CaXP"
      },
      "source": [
        "test_labels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh9yABaME29S"
      },
      "source": [
        "### Verify predictions\n",
        "\n",
        "With the model trained, you can use it to make predictions about some images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Ov9OFDMmOD"
      },
      "source": [
        "Let's look at the 0th image, predictions, and prediction array. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percentage (out of 100) for the predicted label.\n",
        "Plot should look like this:\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV5jw-5HwSmO"
      },
      "source": [
        "#plot the 0th image and next to that plot a bar graph of the predicted values for that image\n",
        "#  if predicted_label is the same as the actual label of the image then the x_axis label should be in blue, whereas \n",
        "# if the predicted_label is not the same as the actual label of the image then the x_axis label should be in red\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wiMlS6wTeM0"
      },
      "source": [
        "Similarly plot the image and predicted values for another value in the test_images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko-uzOufSCSe"
      },
      "source": [
        "#write code below\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgdvGD52CaXR"
      },
      "source": [
        "plot several images with their predictions. Note that the model can be wrong even when very confident."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQlnbqaw2Qu_"
      },
      "source": [
        "# Plot the first 10 test images, their predicted labels, and the true labels.\n",
        "# Color correct predictions in blue and incorrect predictions in red.\n",
        "plt.figure(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R32zteKHCaXT"
      },
      "source": [
        "## Use the trained model\n",
        "\n",
        "Finally, use the trained model to make a prediction about a single image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRJ7JU7JCaXT"
      },
      "source": [
        "# Grab an image from the test dataset.\n",
        "img = test_images[0]\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz3bVp21CaXV"
      },
      "source": [
        "`tf.keras` models are optimized to make predictions on a *batch*, or collection, of examples at once. Accordingly, even though you're using a single image, you need to add it to a list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDFh5yF_CaXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e5b5bc-6a75-4e25-c8c0-b5600fc52561"
      },
      "source": [
        "# Add the image to a batch where it's the only member.\n",
        "#hint: use np.expand_dims\n",
        "#complete code below\n",
        "img = (np.expand_dims(img,0))\n",
        "\n",
        "print(img.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ5wLTkcCaXY"
      },
      "source": [
        "Now predict the correct label for this image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_rzNSdrCaXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80353010-2afa-4533-c000-d41afb7e570f"
      },
      "source": [
        "#complete code below\n",
        "predictions_single = probability_model.predict(img)\n",
        "\n",
        "print(predictions_single)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7.96341283e-07 1.57460008e-07 4.18212842e-09 1.11499705e-10\n",
            "  1.31790490e-08 4.19033784e-03 6.90422226e-08 2.72916984e-02\n",
            "  1.06371601e-07 9.68516827e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU1Y2OAMCaXb"
      },
      "source": [
        "`tf.keras.Model.predict` returns a list of lists—one list for each image in the batch of data. Grab the predictions for our (only) image in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tRmdq_8CaXb"
      },
      "source": [
        "#print the predicted class for the image\n",
        "plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLwk-JKAtWI5"
      },
      "source": [
        "# Regularization\n",
        "In mathematics, statistics, and computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.\n",
        "\n",
        "In order to improve the performance of the model, we use different regularization techniques. There are several techniques, but we will discuss 4 main techniques.\n",
        "\n",
        "1. L1 Regularization\n",
        "2. L2 Regularization\n",
        "3. Dropout\n",
        "4. Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_cp2EXYtWI6"
      },
      "source": [
        "#Basic pre-processing\n",
        "#we're importing the iris dataset from sklearn for our model\n",
        "from sklearn.datasets import load_iris\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "y = to_categorical(y) #converting output to one-hot vector\n",
        "ss = StandardScaler() #standardizing the data\n",
        "X = ss.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=16)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5rUAPyMtWI6"
      },
      "source": [
        "#explore the data (X and y) however you like, (using info(), describe, graphs, etc.) and write any 4 lines on what you understood about the dataset\n",
        "\n",
        "#Solution can be anything\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFc2HbEVCaXd"
      },
      "source": [
        "And the model predicts a label as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UO7lSyatWI6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81095d1d-0dc2-4308-9714-d7fce53d1535"
      },
      "source": [
        "#model building\n",
        "\n",
        "#run the code given below and observe the performance of this simple model having no regularization\n",
        "\n",
        "model1 = Sequential([\n",
        "    Dense(512, activation='tanh', input_shape = X_train[0].shape),\n",
        "    Dense(512//2, activation='tanh'),\n",
        "    Dense(512//4, activation='tanh'),\n",
        "    Dense(512//8, activation='tanh'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "print(model1.summary())\n",
        "model1.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['acc', 'mse'])\n",
        "\n",
        "hist1 = model1.fit(X_train, y_train, epochs=150, batch_size=128, validation_data=(X_test,y_test))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 512)               2560      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 177,219\n",
            "Trainable params: 177,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/150\n",
            "1/1 [==============================] - 1s 644ms/step - loss: 1.0642 - acc: 0.4196 - mse: 0.2156 - val_loss: 1.0219 - val_acc: 0.4211 - val_mse: 0.2044\n",
            "Epoch 2/150\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.9940 - acc: 0.5446 - mse: 0.1986 - val_loss: 0.9693 - val_acc: 0.5000 - val_mse: 0.1917\n",
            "Epoch 3/150\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.9358 - acc: 0.6696 - mse: 0.1843 - val_loss: 0.9244 - val_acc: 0.5789 - val_mse: 0.1809\n",
            "Epoch 4/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.8858 - acc: 0.7321 - mse: 0.1720 - val_loss: 0.8857 - val_acc: 0.6316 - val_mse: 0.1717\n",
            "Epoch 5/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.8423 - acc: 0.7411 - mse: 0.1614 - val_loss: 0.8526 - val_acc: 0.6316 - val_mse: 0.1640\n",
            "Epoch 6/150\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.8043 - acc: 0.7679 - mse: 0.1523 - val_loss: 0.8238 - val_acc: 0.6316 - val_mse: 0.1575\n",
            "Epoch 7/150\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7718 - acc: 0.7768 - mse: 0.1446 - val_loss: 0.7989 - val_acc: 0.6316 - val_mse: 0.1519\n",
            "Epoch 8/150\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7436 - acc: 0.7946 - mse: 0.1380 - val_loss: 0.7770 - val_acc: 0.6316 - val_mse: 0.1472\n",
            "Epoch 9/150\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7187 - acc: 0.8036 - mse: 0.1324 - val_loss: 0.7579 - val_acc: 0.6579 - val_mse: 0.1432\n",
            "Epoch 10/150\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.6965 - acc: 0.8036 - mse: 0.1274 - val_loss: 0.7409 - val_acc: 0.6842 - val_mse: 0.1397\n",
            "Epoch 11/150\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6764 - acc: 0.8214 - mse: 0.1230 - val_loss: 0.7253 - val_acc: 0.7105 - val_mse: 0.1365\n",
            "Epoch 12/150\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6579 - acc: 0.8214 - mse: 0.1191 - val_loss: 0.7110 - val_acc: 0.7368 - val_mse: 0.1338\n",
            "Epoch 13/150\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6410 - acc: 0.8214 - mse: 0.1155 - val_loss: 0.6977 - val_acc: 0.7368 - val_mse: 0.1312\n",
            "Epoch 14/150\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6255 - acc: 0.8214 - mse: 0.1123 - val_loss: 0.6855 - val_acc: 0.7368 - val_mse: 0.1290\n",
            "Epoch 15/150\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6111 - acc: 0.8304 - mse: 0.1094 - val_loss: 0.6742 - val_acc: 0.7368 - val_mse: 0.1269\n",
            "Epoch 16/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5979 - acc: 0.8393 - mse: 0.1068 - val_loss: 0.6639 - val_acc: 0.7368 - val_mse: 0.1251\n",
            "Epoch 17/150\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5858 - acc: 0.8393 - mse: 0.1044 - val_loss: 0.6545 - val_acc: 0.7368 - val_mse: 0.1235\n",
            "Epoch 18/150\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.5746 - acc: 0.8393 - mse: 0.1023 - val_loss: 0.6459 - val_acc: 0.7368 - val_mse: 0.1221\n",
            "Epoch 19/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.5641 - acc: 0.8393 - mse: 0.1003 - val_loss: 0.6378 - val_acc: 0.7368 - val_mse: 0.1207\n",
            "Epoch 20/150\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.5543 - acc: 0.8393 - mse: 0.0984 - val_loss: 0.6303 - val_acc: 0.7368 - val_mse: 0.1196\n",
            "Epoch 21/150\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5451 - acc: 0.8393 - mse: 0.0967 - val_loss: 0.6232 - val_acc: 0.7368 - val_mse: 0.1184\n",
            "Epoch 22/150\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.5364 - acc: 0.8393 - mse: 0.0951 - val_loss: 0.6164 - val_acc: 0.7368 - val_mse: 0.1174\n",
            "Epoch 23/150\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.5281 - acc: 0.8393 - mse: 0.0936 - val_loss: 0.6100 - val_acc: 0.7368 - val_mse: 0.1164\n",
            "Epoch 24/150\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.5202 - acc: 0.8393 - mse: 0.0922 - val_loss: 0.6038 - val_acc: 0.7368 - val_mse: 0.1155\n",
            "Epoch 25/150\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.5125 - acc: 0.8393 - mse: 0.0908 - val_loss: 0.5979 - val_acc: 0.7368 - val_mse: 0.1146\n",
            "Epoch 26/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5052 - acc: 0.8482 - mse: 0.0894 - val_loss: 0.5923 - val_acc: 0.7368 - val_mse: 0.1137\n",
            "Epoch 27/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4981 - acc: 0.8482 - mse: 0.0882 - val_loss: 0.5869 - val_acc: 0.7368 - val_mse: 0.1130\n",
            "Epoch 28/150\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4914 - acc: 0.8482 - mse: 0.0870 - val_loss: 0.5816 - val_acc: 0.7368 - val_mse: 0.1122\n",
            "Epoch 29/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4849 - acc: 0.8482 - mse: 0.0858 - val_loss: 0.5765 - val_acc: 0.7368 - val_mse: 0.1115\n",
            "Epoch 30/150\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4787 - acc: 0.8482 - mse: 0.0847 - val_loss: 0.5717 - val_acc: 0.7368 - val_mse: 0.1107\n",
            "Epoch 31/150\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4727 - acc: 0.8482 - mse: 0.0837 - val_loss: 0.5670 - val_acc: 0.7368 - val_mse: 0.1101\n",
            "Epoch 32/150\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4670 - acc: 0.8482 - mse: 0.0827 - val_loss: 0.5624 - val_acc: 0.7368 - val_mse: 0.1094\n",
            "Epoch 33/150\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4616 - acc: 0.8482 - mse: 0.0817 - val_loss: 0.5581 - val_acc: 0.7368 - val_mse: 0.1088\n",
            "Epoch 34/150\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4563 - acc: 0.8482 - mse: 0.0808 - val_loss: 0.5540 - val_acc: 0.7368 - val_mse: 0.1082\n",
            "Epoch 35/150\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.4512 - acc: 0.8482 - mse: 0.0799 - val_loss: 0.5500 - val_acc: 0.7368 - val_mse: 0.1077\n",
            "Epoch 36/150\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4464 - acc: 0.8482 - mse: 0.0790 - val_loss: 0.5462 - val_acc: 0.7368 - val_mse: 0.1071\n",
            "Epoch 37/150\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4417 - acc: 0.8482 - mse: 0.0782 - val_loss: 0.5424 - val_acc: 0.7368 - val_mse: 0.1066\n",
            "Epoch 38/150\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4372 - acc: 0.8482 - mse: 0.0774 - val_loss: 0.5388 - val_acc: 0.7632 - val_mse: 0.1061\n",
            "Epoch 39/150\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.4328 - acc: 0.8482 - mse: 0.0767 - val_loss: 0.5353 - val_acc: 0.7632 - val_mse: 0.1056\n",
            "Epoch 40/150\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4285 - acc: 0.8482 - mse: 0.0759 - val_loss: 0.5319 - val_acc: 0.7632 - val_mse: 0.1052\n",
            "Epoch 41/150\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4244 - acc: 0.8661 - mse: 0.0752 - val_loss: 0.5286 - val_acc: 0.7632 - val_mse: 0.1047\n",
            "Epoch 42/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4204 - acc: 0.8661 - mse: 0.0745 - val_loss: 0.5254 - val_acc: 0.7632 - val_mse: 0.1043\n",
            "Epoch 43/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4164 - acc: 0.8661 - mse: 0.0738 - val_loss: 0.5222 - val_acc: 0.7368 - val_mse: 0.1038\n",
            "Epoch 44/150\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4126 - acc: 0.8661 - mse: 0.0731 - val_loss: 0.5190 - val_acc: 0.7368 - val_mse: 0.1034\n",
            "Epoch 45/150\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4089 - acc: 0.8661 - mse: 0.0725 - val_loss: 0.5160 - val_acc: 0.7368 - val_mse: 0.1030\n",
            "Epoch 46/150\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.4052 - acc: 0.8661 - mse: 0.0718 - val_loss: 0.5129 - val_acc: 0.7368 - val_mse: 0.1025\n",
            "Epoch 47/150\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4016 - acc: 0.8661 - mse: 0.0712 - val_loss: 0.5100 - val_acc: 0.7368 - val_mse: 0.1021\n",
            "Epoch 48/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3981 - acc: 0.8661 - mse: 0.0706 - val_loss: 0.5071 - val_acc: 0.7368 - val_mse: 0.1017\n",
            "Epoch 49/150\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3947 - acc: 0.8750 - mse: 0.0700 - val_loss: 0.5043 - val_acc: 0.7368 - val_mse: 0.1014\n",
            "Epoch 50/150\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.3914 - acc: 0.8750 - mse: 0.0694 - val_loss: 0.5016 - val_acc: 0.7368 - val_mse: 0.1010\n",
            "Epoch 51/150\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.3882 - acc: 0.8750 - mse: 0.0689 - val_loss: 0.4990 - val_acc: 0.7368 - val_mse: 0.1006\n",
            "Epoch 52/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3850 - acc: 0.8750 - mse: 0.0683 - val_loss: 0.4965 - val_acc: 0.7368 - val_mse: 0.1003\n",
            "Epoch 53/150\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3819 - acc: 0.8750 - mse: 0.0678 - val_loss: 0.4940 - val_acc: 0.7368 - val_mse: 0.1000\n",
            "Epoch 54/150\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.3789 - acc: 0.8750 - mse: 0.0672 - val_loss: 0.4916 - val_acc: 0.7368 - val_mse: 0.0996\n",
            "Epoch 55/150\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3760 - acc: 0.8750 - mse: 0.0667 - val_loss: 0.4893 - val_acc: 0.7368 - val_mse: 0.0993\n",
            "Epoch 56/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3731 - acc: 0.8750 - mse: 0.0662 - val_loss: 0.4870 - val_acc: 0.7632 - val_mse: 0.0990\n",
            "Epoch 57/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.3703 - acc: 0.8750 - mse: 0.0657 - val_loss: 0.4847 - val_acc: 0.7632 - val_mse: 0.0987\n",
            "Epoch 58/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.3676 - acc: 0.8750 - mse: 0.0652 - val_loss: 0.4826 - val_acc: 0.7632 - val_mse: 0.0984\n",
            "Epoch 59/150\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.3649 - acc: 0.8750 - mse: 0.0648 - val_loss: 0.4804 - val_acc: 0.7632 - val_mse: 0.0981\n",
            "Epoch 60/150\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.3623 - acc: 0.8750 - mse: 0.0643 - val_loss: 0.4783 - val_acc: 0.7632 - val_mse: 0.0978\n",
            "Epoch 61/150\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.3597 - acc: 0.8750 - mse: 0.0638 - val_loss: 0.4762 - val_acc: 0.7632 - val_mse: 0.0975\n",
            "Epoch 62/150\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3571 - acc: 0.8750 - mse: 0.0634 - val_loss: 0.4742 - val_acc: 0.7632 - val_mse: 0.0972\n",
            "Epoch 63/150\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3547 - acc: 0.8750 - mse: 0.0629 - val_loss: 0.4722 - val_acc: 0.7632 - val_mse: 0.0969\n",
            "Epoch 64/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3522 - acc: 0.8750 - mse: 0.0625 - val_loss: 0.4702 - val_acc: 0.7632 - val_mse: 0.0966\n",
            "Epoch 65/150\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.3498 - acc: 0.8750 - mse: 0.0620 - val_loss: 0.4683 - val_acc: 0.7632 - val_mse: 0.0964\n",
            "Epoch 66/150\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3475 - acc: 0.8750 - mse: 0.0616 - val_loss: 0.4664 - val_acc: 0.7632 - val_mse: 0.0961\n",
            "Epoch 67/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3452 - acc: 0.8750 - mse: 0.0612 - val_loss: 0.4646 - val_acc: 0.7632 - val_mse: 0.0958\n",
            "Epoch 68/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.3429 - acc: 0.8750 - mse: 0.0608 - val_loss: 0.4627 - val_acc: 0.7632 - val_mse: 0.0955\n",
            "Epoch 69/150\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3407 - acc: 0.8839 - mse: 0.0604 - val_loss: 0.4609 - val_acc: 0.7632 - val_mse: 0.0953\n",
            "Epoch 70/150\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3385 - acc: 0.8839 - mse: 0.0600 - val_loss: 0.4590 - val_acc: 0.7632 - val_mse: 0.0950\n",
            "Epoch 71/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3363 - acc: 0.8839 - mse: 0.0596 - val_loss: 0.4572 - val_acc: 0.7632 - val_mse: 0.0947\n",
            "Epoch 72/150\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3341 - acc: 0.8839 - mse: 0.0592 - val_loss: 0.4554 - val_acc: 0.7632 - val_mse: 0.0944\n",
            "Epoch 73/150\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.3320 - acc: 0.8839 - mse: 0.0588 - val_loss: 0.4536 - val_acc: 0.7632 - val_mse: 0.0941\n",
            "Epoch 74/150\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.3299 - acc: 0.8929 - mse: 0.0584 - val_loss: 0.4518 - val_acc: 0.7632 - val_mse: 0.0939\n",
            "Epoch 75/150\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3278 - acc: 0.8929 - mse: 0.0580 - val_loss: 0.4501 - val_acc: 0.7632 - val_mse: 0.0936\n",
            "Epoch 76/150\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.3258 - acc: 0.8929 - mse: 0.0576 - val_loss: 0.4485 - val_acc: 0.7632 - val_mse: 0.0933\n",
            "Epoch 77/150\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3237 - acc: 0.8929 - mse: 0.0572 - val_loss: 0.4467 - val_acc: 0.7632 - val_mse: 0.0931\n",
            "Epoch 78/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3217 - acc: 0.8929 - mse: 0.0569 - val_loss: 0.4450 - val_acc: 0.7632 - val_mse: 0.0928\n",
            "Epoch 79/150\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3198 - acc: 0.8929 - mse: 0.0565 - val_loss: 0.4434 - val_acc: 0.7632 - val_mse: 0.0925\n",
            "Epoch 80/150\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.3178 - acc: 0.8929 - mse: 0.0561 - val_loss: 0.4417 - val_acc: 0.7632 - val_mse: 0.0923\n",
            "Epoch 81/150\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3159 - acc: 0.8929 - mse: 0.0558 - val_loss: 0.4401 - val_acc: 0.7632 - val_mse: 0.0920\n",
            "Epoch 82/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3140 - acc: 0.8929 - mse: 0.0554 - val_loss: 0.4385 - val_acc: 0.7632 - val_mse: 0.0917\n",
            "Epoch 83/150\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3121 - acc: 0.8929 - mse: 0.0551 - val_loss: 0.4369 - val_acc: 0.7632 - val_mse: 0.0915\n",
            "Epoch 84/150\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3103 - acc: 0.8929 - mse: 0.0547 - val_loss: 0.4353 - val_acc: 0.7632 - val_mse: 0.0912\n",
            "Epoch 85/150\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3084 - acc: 0.8929 - mse: 0.0544 - val_loss: 0.4338 - val_acc: 0.7632 - val_mse: 0.0910\n",
            "Epoch 86/150\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3067 - acc: 0.8929 - mse: 0.0540 - val_loss: 0.4323 - val_acc: 0.7632 - val_mse: 0.0907\n",
            "Epoch 87/150\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3049 - acc: 0.9018 - mse: 0.0537 - val_loss: 0.4308 - val_acc: 0.7632 - val_mse: 0.0905\n",
            "Epoch 88/150\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3031 - acc: 0.9018 - mse: 0.0534 - val_loss: 0.4294 - val_acc: 0.7632 - val_mse: 0.0903\n",
            "Epoch 89/150\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3014 - acc: 0.9107 - mse: 0.0530 - val_loss: 0.4279 - val_acc: 0.7632 - val_mse: 0.0900\n",
            "Epoch 90/150\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2997 - acc: 0.9107 - mse: 0.0527 - val_loss: 0.4265 - val_acc: 0.7632 - val_mse: 0.0898\n",
            "Epoch 91/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2980 - acc: 0.9107 - mse: 0.0524 - val_loss: 0.4250 - val_acc: 0.7632 - val_mse: 0.0896\n",
            "Epoch 92/150\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2963 - acc: 0.9107 - mse: 0.0521 - val_loss: 0.4236 - val_acc: 0.7632 - val_mse: 0.0893\n",
            "Epoch 93/150\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2946 - acc: 0.9107 - mse: 0.0518 - val_loss: 0.4222 - val_acc: 0.7632 - val_mse: 0.0891\n",
            "Epoch 94/150\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2930 - acc: 0.9107 - mse: 0.0515 - val_loss: 0.4208 - val_acc: 0.7632 - val_mse: 0.0888\n",
            "Epoch 95/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2913 - acc: 0.9196 - mse: 0.0511 - val_loss: 0.4195 - val_acc: 0.7632 - val_mse: 0.0886\n",
            "Epoch 96/150\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2897 - acc: 0.9196 - mse: 0.0508 - val_loss: 0.4181 - val_acc: 0.7632 - val_mse: 0.0884\n",
            "Epoch 97/150\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2881 - acc: 0.9196 - mse: 0.0505 - val_loss: 0.4168 - val_acc: 0.7632 - val_mse: 0.0882\n",
            "Epoch 98/150\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2865 - acc: 0.9196 - mse: 0.0502 - val_loss: 0.4154 - val_acc: 0.7632 - val_mse: 0.0879\n",
            "Epoch 99/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2850 - acc: 0.9196 - mse: 0.0499 - val_loss: 0.4141 - val_acc: 0.7895 - val_mse: 0.0877\n",
            "Epoch 100/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2834 - acc: 0.9196 - mse: 0.0496 - val_loss: 0.4127 - val_acc: 0.7895 - val_mse: 0.0875\n",
            "Epoch 101/150\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2819 - acc: 0.9196 - mse: 0.0493 - val_loss: 0.4114 - val_acc: 0.7895 - val_mse: 0.0872\n",
            "Epoch 102/150\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.2804 - acc: 0.9196 - mse: 0.0490 - val_loss: 0.4101 - val_acc: 0.7895 - val_mse: 0.0870\n",
            "Epoch 103/150\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2789 - acc: 0.9196 - mse: 0.0488 - val_loss: 0.4088 - val_acc: 0.7895 - val_mse: 0.0868\n",
            "Epoch 104/150\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2774 - acc: 0.9196 - mse: 0.0485 - val_loss: 0.4075 - val_acc: 0.7895 - val_mse: 0.0865\n",
            "Epoch 105/150\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2759 - acc: 0.9196 - mse: 0.0482 - val_loss: 0.4062 - val_acc: 0.7895 - val_mse: 0.0863\n",
            "Epoch 106/150\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2744 - acc: 0.9196 - mse: 0.0479 - val_loss: 0.4049 - val_acc: 0.7895 - val_mse: 0.0861\n",
            "Epoch 107/150\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2730 - acc: 0.9196 - mse: 0.0476 - val_loss: 0.4036 - val_acc: 0.7895 - val_mse: 0.0859\n",
            "Epoch 108/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2716 - acc: 0.9196 - mse: 0.0474 - val_loss: 0.4023 - val_acc: 0.7895 - val_mse: 0.0856\n",
            "Epoch 109/150\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2701 - acc: 0.9196 - mse: 0.0471 - val_loss: 0.4010 - val_acc: 0.7895 - val_mse: 0.0854\n",
            "Epoch 110/150\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2687 - acc: 0.9196 - mse: 0.0468 - val_loss: 0.3997 - val_acc: 0.7895 - val_mse: 0.0852\n",
            "Epoch 111/150\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2673 - acc: 0.9286 - mse: 0.0466 - val_loss: 0.3984 - val_acc: 0.7895 - val_mse: 0.0849\n",
            "Epoch 112/150\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2659 - acc: 0.9286 - mse: 0.0463 - val_loss: 0.3972 - val_acc: 0.7895 - val_mse: 0.0847\n",
            "Epoch 113/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2645 - acc: 0.9286 - mse: 0.0460 - val_loss: 0.3959 - val_acc: 0.7895 - val_mse: 0.0845\n",
            "Epoch 114/150\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2631 - acc: 0.9286 - mse: 0.0457 - val_loss: 0.3946 - val_acc: 0.7895 - val_mse: 0.0842\n",
            "Epoch 115/150\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2618 - acc: 0.9286 - mse: 0.0455 - val_loss: 0.3933 - val_acc: 0.7895 - val_mse: 0.0840\n",
            "Epoch 116/150\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2604 - acc: 0.9286 - mse: 0.0452 - val_loss: 0.3920 - val_acc: 0.7895 - val_mse: 0.0837\n",
            "Epoch 117/150\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2591 - acc: 0.9286 - mse: 0.0449 - val_loss: 0.3908 - val_acc: 0.7895 - val_mse: 0.0835\n",
            "Epoch 118/150\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2577 - acc: 0.9286 - mse: 0.0447 - val_loss: 0.3895 - val_acc: 0.7895 - val_mse: 0.0832\n",
            "Epoch 119/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2564 - acc: 0.9286 - mse: 0.0444 - val_loss: 0.3882 - val_acc: 0.7895 - val_mse: 0.0830\n",
            "Epoch 120/150\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2551 - acc: 0.9286 - mse: 0.0441 - val_loss: 0.3869 - val_acc: 0.7895 - val_mse: 0.0828\n",
            "Epoch 121/150\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2538 - acc: 0.9286 - mse: 0.0439 - val_loss: 0.3856 - val_acc: 0.7895 - val_mse: 0.0825\n",
            "Epoch 122/150\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.2525 - acc: 0.9286 - mse: 0.0436 - val_loss: 0.3844 - val_acc: 0.7895 - val_mse: 0.0823\n",
            "Epoch 123/150\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2512 - acc: 0.9286 - mse: 0.0434 - val_loss: 0.3831 - val_acc: 0.7895 - val_mse: 0.0820\n",
            "Epoch 124/150\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2499 - acc: 0.9286 - mse: 0.0431 - val_loss: 0.3818 - val_acc: 0.7895 - val_mse: 0.0817\n",
            "Epoch 125/150\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2486 - acc: 0.9375 - mse: 0.0429 - val_loss: 0.3805 - val_acc: 0.7895 - val_mse: 0.0815\n",
            "Epoch 126/150\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2473 - acc: 0.9375 - mse: 0.0426 - val_loss: 0.3793 - val_acc: 0.7895 - val_mse: 0.0812\n",
            "Epoch 127/150\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2461 - acc: 0.9375 - mse: 0.0423 - val_loss: 0.3780 - val_acc: 0.7895 - val_mse: 0.0810\n",
            "Epoch 128/150\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2448 - acc: 0.9375 - mse: 0.0421 - val_loss: 0.3767 - val_acc: 0.7895 - val_mse: 0.0807\n",
            "Epoch 129/150\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2436 - acc: 0.9375 - mse: 0.0418 - val_loss: 0.3755 - val_acc: 0.7895 - val_mse: 0.0805\n",
            "Epoch 130/150\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.2423 - acc: 0.9375 - mse: 0.0416 - val_loss: 0.3742 - val_acc: 0.7895 - val_mse: 0.0802\n",
            "Epoch 131/150\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.2411 - acc: 0.9375 - mse: 0.0413 - val_loss: 0.3730 - val_acc: 0.7895 - val_mse: 0.0800\n",
            "Epoch 132/150\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2399 - acc: 0.9375 - mse: 0.0411 - val_loss: 0.3717 - val_acc: 0.7895 - val_mse: 0.0797\n",
            "Epoch 133/150\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.2386 - acc: 0.9375 - mse: 0.0408 - val_loss: 0.3705 - val_acc: 0.7895 - val_mse: 0.0794\n",
            "Epoch 134/150\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2374 - acc: 0.9375 - mse: 0.0406 - val_loss: 0.3692 - val_acc: 0.7895 - val_mse: 0.0792\n",
            "Epoch 135/150\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2362 - acc: 0.9375 - mse: 0.0404 - val_loss: 0.3679 - val_acc: 0.7895 - val_mse: 0.0789\n",
            "Epoch 136/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2350 - acc: 0.9375 - mse: 0.0401 - val_loss: 0.3666 - val_acc: 0.7895 - val_mse: 0.0787\n",
            "Epoch 137/150\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2338 - acc: 0.9375 - mse: 0.0399 - val_loss: 0.3653 - val_acc: 0.7895 - val_mse: 0.0784\n",
            "Epoch 138/150\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2326 - acc: 0.9375 - mse: 0.0396 - val_loss: 0.3640 - val_acc: 0.7895 - val_mse: 0.0781\n",
            "Epoch 139/150\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2314 - acc: 0.9375 - mse: 0.0394 - val_loss: 0.3627 - val_acc: 0.7895 - val_mse: 0.0778\n",
            "Epoch 140/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2302 - acc: 0.9375 - mse: 0.0391 - val_loss: 0.3613 - val_acc: 0.7895 - val_mse: 0.0775\n",
            "Epoch 141/150\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.2290 - acc: 0.9375 - mse: 0.0389 - val_loss: 0.3600 - val_acc: 0.7895 - val_mse: 0.0773\n",
            "Epoch 142/150\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2278 - acc: 0.9375 - mse: 0.0387 - val_loss: 0.3587 - val_acc: 0.7895 - val_mse: 0.0770\n",
            "Epoch 143/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2265 - acc: 0.9375 - mse: 0.0384 - val_loss: 0.3573 - val_acc: 0.7895 - val_mse: 0.0767\n",
            "Epoch 144/150\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2253 - acc: 0.9375 - mse: 0.0382 - val_loss: 0.3561 - val_acc: 0.7895 - val_mse: 0.0764\n",
            "Epoch 145/150\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2241 - acc: 0.9375 - mse: 0.0379 - val_loss: 0.3549 - val_acc: 0.7895 - val_mse: 0.0762\n",
            "Epoch 146/150\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2229 - acc: 0.9375 - mse: 0.0377 - val_loss: 0.3537 - val_acc: 0.7895 - val_mse: 0.0759\n",
            "Epoch 147/150\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2217 - acc: 0.9375 - mse: 0.0374 - val_loss: 0.3525 - val_acc: 0.7895 - val_mse: 0.0757\n",
            "Epoch 148/150\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2205 - acc: 0.9375 - mse: 0.0372 - val_loss: 0.3513 - val_acc: 0.7895 - val_mse: 0.0754\n",
            "Epoch 149/150\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2193 - acc: 0.9375 - mse: 0.0369 - val_loss: 0.3501 - val_acc: 0.8158 - val_mse: 0.0752\n",
            "Epoch 150/150\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2181 - acc: 0.9375 - mse: 0.0367 - val_loss: 0.3489 - val_acc: 0.8158 - val_mse: 0.0749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2n1tKmytWI6"
      },
      "source": [
        "#After training the model, evaluate the model and find the  loss, accuracy and mse on the test set using the evaluate() function\n",
        "#print the loss, accuracy and MSE\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojzZbA0ntWI6"
      },
      "source": [
        "#Observe the plots for Validation Loss and Training Loss.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "plt.plot(hist1.history['loss'], label = 'loss')\n",
        "plt.plot(hist1.history['val_loss'], label='val loss')\n",
        "plt.title(\"Train Loss vs Val_Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# we observe that validation loss is increasing as compared to training loss. \n",
        "# This increase in training loss shows that our model is overfitted.\n",
        "\n",
        "#similarly we plot for model accuracy \n",
        "plt.plot(hist1.history['acc'], label = 'train acc')\n",
        "plt.plot(hist1.history['val_acc'], label='val acc')\n",
        "plt.title(\"train acc vs Val_acc\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"acc\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#This again shows that validation accuracy is low as compared to training accuracy, which again shows signs of overfitting. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjz06gfttWI6"
      },
      "source": [
        "\n",
        "\n",
        "## L1 Regularization:\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eUojGfBtWI6"
      },
      "source": [
        "\n",
        "#A commonly used Regularization technique is L1 regularization, also known as Lasso Regularization.\n",
        "\n",
        "#The main concept of L1 Regularization is that we have to penalize our weights by adding absolute values of weight in our loss function, multiplied by a regularization parameter lambda λ, where λ is manually tuned to be greater than 0.\n",
        "\n",
        "\n",
        "\n",
        "#to implement L1 regularization, create another model called model2 similar to model1, this time add an extra parameter kernel_regularizer, which we set it to ‘l1’ for L1 Regularization in the first Dense layer:\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Om_ivOtWI6"
      },
      "source": [
        "\n",
        "\n",
        "#just how we used evaluate() in the basic model previously, use it again to find the loss, accuracy and MSE on the test set\n",
        "\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqaG5WkGtWI6"
      },
      "source": [
        "# Plot the train loss vs validation loss and train accuracy vs validation accuracy graphs just like we did before\n",
        "\n",
        "\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sujos4Z3tWI6"
      },
      "source": [
        "\n",
        "# create a new model model3, similar to model1 and model2 and add l1 in more layers to check if it improves the model or not.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCbYzZ7rtWI6"
      },
      "source": [
        "\n",
        "#just how we used evaluate() in the basic model previously, use it again to find the loss, accuracy and MSE on the test set\n",
        "\n",
        "\n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKM2LLXYtWI7"
      },
      "source": [
        "# Plot the train loss vs validation loss and train accuracy vs validation accuracy graphs just like we did before\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYxPyGoYtWI7"
      },
      "source": [
        "\n",
        "## L2 Regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK5t2PRTtWI7"
      },
      "source": [
        " \n",
        "\n",
        "# L2 Regularization is another regularization technique which is also known as Ridge regularization. In L2 regularization we add the squared magnitude of weights to penalize our lost function.\n",
        "\n",
        "#to implement L2 regularization, create another model called model4 similar to our initial basic model, this time add an extra parameter kernel_regularizer, which we set to ‘l2’ for L1 Regularization in the first Dense layer.\n",
        "#compile and run the model\n",
        "\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj2ms-9ztWI7"
      },
      "source": [
        "\n",
        "#just how we used evaluate() in the basic model previously, use it again to find the loss, accuracy and MSE on the test set\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9oV0BRftWI7"
      },
      "source": [
        "\n",
        "# Plot the train loss vs validation loss and train accuracy vs validation accuracy graphs just like we did before\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3QFkLTytWI7"
      },
      "source": [
        "\n",
        "# Now create model5 having similar to the above models having L2 in all other layers.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xSBWqgXtWI7"
      },
      "source": [
        "\n",
        "#just how we used evaluate() in the basic model previously, use it again to find the loss, accuracy and MSE on the test set\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zMva1VGtWI7"
      },
      "source": [
        "# Plot the train loss vs validation loss and train accuracy vs validation accuracy graphs just like we did before\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-C_KqDAtWI7"
      },
      "source": [
        "\n",
        " \n",
        "\n",
        "## Dropout\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_eXOYdbtWI7"
      },
      "source": [
        "\n",
        "#Another common way to avoid regularization is by using the Dropout technique. The main idea behind using dropout is that we randomly turn off some neurons in our layer based on some probability.\n",
        "\n",
        "# Create a model6 having same layers as the basic model. Add a dropout layer in this model, lets say after the 1st dense layer, and set dropout rate as 50%\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3QknWrPtWI7"
      },
      "source": [
        "#just how we used evaluate() in the basic model previously, use it again to find the loss, accuracy and MSE on the test set\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fj9EEgbtWI7"
      },
      "source": [
        "\n",
        "# Plot the train loss vs validation loss and train accuracy vs validation accuracy graphs just like we did before\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uK4Yo3ctWI7"
      },
      "source": [
        "# Create model7, having more dropout layers, having a dropout rate of your choice\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl5SZG7PtWI8"
      },
      "source": [
        "#just how we used evaluate() in the basic model previously, use it again to find the loss, accuracy and MSE on the test set\n",
        "\n",
        "\n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dajge1vctWI8"
      },
      "source": [
        "# Plot the train loss vs validation loss and train accuracy vs validation accuracy graphs just like we did before\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KXo9irZtWI8"
      },
      "source": [
        "## Batch Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzF8Edv5tWI8"
      },
      "source": [
        "\n",
        "# The main idea behind batch normalization is that we normalize the input layer by using several techniques (sklearn.preprocessing.StandardScaler) in our case, which improves the model performance, so if the input layer is benefitted by normalization, why not normalize the hidden layers, which will improve and fasten learning even further.\n",
        "\n",
        "# To add it in your TensorFlow model, just add tf.keras.layers.BatchNormalization() after your layers.\n",
        "\n",
        "#Create model8, having same layers as the first model, but this time add a BatchNormalization layer after the first or second dense layer\n",
        "\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ28fQYStWI8"
      },
      "source": [
        "#just how we used evaluate() in the basic model previously, use it again to find the loss, accuracy and MSE on the test set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlfTXH_8tWI8"
      },
      "source": [
        "# Plot the train loss vs validation loss and train accuracy vs validation accuracy graphs just like we did before\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSEY6abutWI8"
      },
      "source": [
        "\n",
        "# Now create model9, having a BatchNormalization layer after each dense layer\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIKJ0M9GtWI8"
      },
      "source": [
        "#just how we used evaluate() in the basic model previously, use it again to find the loss, accuracy and MSE on the test set\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5iDPwultWI8"
      },
      "source": [
        "# Plot the train loss vs validation loss and train accuracy vs validation accuracy graphs just like we did before\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb2YmGJAtWI8"
      },
      "source": [
        "#After completing the above tasks, note down what you've understood and observed in each of the 9 models and what type of regularization works best for this dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}