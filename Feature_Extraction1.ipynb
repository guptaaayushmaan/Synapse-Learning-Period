{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "Feature_Extraction.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVDn1I0oZ6i9"
      },
      "source": [
        "<a id=\"Introduction\"></a>\n",
        "# Introduction\n",
        "\n",
        "This week we will be focusing on NLP preprocessing and feature extraction techniques and codes. We will mainly use the [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) dataset for illustration.\n",
        "\n",
        "We will revise our concepts learned from last week and will be learning newer concepts based on feature extraction and word embeddings\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1750/1*rJQVqDjbhI3k22lHqa4dFw.png\" align=\"center\"/>\n",
        "\n",
        "image source: [Natural Language Processing Pipeline](https://towardsdatascience.com/natural-language-processing-pipeline-93df02ecd03f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0rd_8LHZ6i-"
      },
      "source": [
        "<a id=\"Read_and_explore_data\"></a>\n",
        "\n",
        "# Read and explore data\n",
        "\n",
        "<a id=\"Importing_Main_Packages\"></a>\n",
        "## Importing Main Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:32.194579Z",
          "iopub.status.busy": "2021-11-13T10:20:32.194211Z",
          "iopub.status.idle": "2021-11-13T10:20:32.234873Z",
          "shell.execute_reply": "2021-11-13T10:20:32.234227Z",
          "shell.execute_reply.started": "2021-11-13T10:20:32.194548Z"
        },
        "scrolled": true,
        "id": "mlLkwB1PZ6i_"
      },
      "source": [
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "# Libraries and packages for text (pre-)processing \n",
        "import string\n",
        "import re\n",
        "import nltk\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfDqe2pqZ6jA"
      },
      "source": [
        "<a id=\"Read_the_Data\"></a>\n",
        "## Read the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:32.236818Z",
          "iopub.status.busy": "2021-11-13T10:20:32.236328Z",
          "iopub.status.idle": "2021-11-13T10:20:32.272601Z",
          "shell.execute_reply": "2021-11-13T10:20:32.271053Z",
          "shell.execute_reply.started": "2021-11-13T10:20:32.236782Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MPM6CxVOZ6jA",
        "outputId": "c9ed77e4-ac32-49a2-b00a-2c5a1d07c6e4"
      },
      "source": [
        "# read the \"train.csv\" file and display using head(). Name the variable as \"train_df\"\n",
        "train_df = pd.read_csv('/train.csv');\n",
        "train_df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:32.274127Z",
          "iopub.status.busy": "2021-11-13T10:20:32.273855Z",
          "iopub.status.idle": "2021-11-13T10:20:32.296992Z",
          "shell.execute_reply": "2021-11-13T10:20:32.295936Z",
          "shell.execute_reply.started": "2021-11-13T10:20:32.274099Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "ddLpJpKQZ6jA",
        "outputId": "9410c043-1e1f-4d64-d695-cdddd8b87a4b"
      },
      "source": [
        "# some data exploration\n",
        "display(train_df[~train_df[\"location\"].isnull()].head())\n",
        "display(train_df[train_df[\"target\"] == 0][\"text\"].values[1])\n",
        "display(train_df[train_df[\"target\"] == 1][\"text\"].values[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>48</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Birmingham</td>\n",
              "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>49</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Est. September 2012 - Bristol</td>\n",
              "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>50</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>AFRICA</td>\n",
              "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>52</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Philadelphia, PA</td>\n",
              "      <td>Crying out for more! Set me ablaze</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>53</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>London, UK</td>\n",
              "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id keyword  ...                                               text target\n",
              "31  48  ablaze  ...  @bbcmtd Wholesale Markets ablaze http://t.co/l...      1\n",
              "32  49  ablaze  ...  We always try to bring the heavy. #metal #RT h...      0\n",
              "33  50  ablaze  ...  #AFRICANBAZE: Breaking news:Nigeria flag set a...      1\n",
              "34  52  ablaze  ...                 Crying out for more! Set me ablaze      0\n",
              "35  53  ablaze  ...  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I love fruits'"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Forest fire near La Ronge Sask. Canada'"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfdREDaJZ6jA"
      },
      "source": [
        "<a id=\"Text_Preprocessing\"></a>\n",
        "\n",
        "# Text Preprocessing:\n",
        "\n",
        "<a id=\"Capitalization\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:32.299060Z",
          "iopub.status.busy": "2021-11-13T10:20:32.298617Z",
          "iopub.status.idle": "2021-11-13T10:20:32.319369Z",
          "shell.execute_reply": "2021-11-13T10:20:32.318525Z",
          "shell.execute_reply.started": "2021-11-13T10:20:32.299030Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "I3uRyBz4Z6jA",
        "outputId": "98bb1201-c2b3-4e44-f9ba-5e531ef871ab"
      },
      "source": [
        "#Write the code to remove all text capitalization and save it in a column called \"text_clean\"\n",
        "# Complete the code below\n",
        "# Hint: Use pandas apply()\n",
        "train_df['text_clean']=train_df['text'].str.lower()\n",
        "\n",
        "\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>our deeds are the reason of this #earthquake m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>forest fire near la ronge sask. canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>all residents asked to 'shelter in place' are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ... target                                         text_clean\n",
              "0   1     NaN  ...      1  our deeds are the reason of this #earthquake m...\n",
              "1   4     NaN  ...      1             forest fire near la ronge sask. canada\n",
              "2   5     NaN  ...      1  all residents asked to 'shelter in place' are ...\n",
              "3   6     NaN  ...      1  13,000 people receive #wildfires evacuation or...\n",
              "4   7     NaN  ...      1  just got sent this photo from ruby #alaska as ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh-jTEsPZ6jB"
      },
      "source": [
        "## Noise Removal \n",
        "Text data could include various unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters, or other special characters (symbols, emojis, and other graphic characters). \n",
        "\n",
        "### Remove URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:39.762075Z",
          "iopub.status.busy": "2021-11-13T10:20:39.761848Z",
          "iopub.status.idle": "2021-11-13T10:20:39.766745Z",
          "shell.execute_reply": "2021-11-13T10:20:39.765345Z",
          "shell.execute_reply.started": "2021-11-13T10:20:39.762051Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN0PebEWZ6jB",
        "outputId": "e23453d4-f1a1-45f2-b263-328ebd33418a"
      },
      "source": [
        "#complete the function below to remove urls\n",
        "\n",
        "def remove_URL(text):\n",
        "    \"\"\"\n",
        "        Remove URLs from a sample string\n",
        "    \"\"\"\n",
        "    import re\n",
        "    text = re.sub(r'http\\S+', '', str(text))\n",
        "    return text\n",
        "\n",
        "# removing urls from the text\n",
        "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_URL(x))\n",
        "\n",
        "# double check\n",
        "print(train_df[\"text\"][31])\n",
        "print(train_df[\"text_clean\"][31])\n",
        "print(train_df[\"text\"][37])\n",
        "print(train_df[\"text_clean\"][37])\n",
        "print(train_df[\"text\"][62])\n",
        "print(train_df[\"text_clean\"][62])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n",
            "@bbcmtd wholesale markets ablaze \n",
            "INEC Office in Abia Set Ablaze - http://t.co/3ImaomknnA\n",
            "inec office in abia set ablaze - \n",
            "Rene Ablaze &amp; Jacinta - Secret 2k13 (Fallen Skies Edit) - Mar 30 2013  https://t.co/7MLMsUzV1Z\n",
            "rene ablaze &amp; jacinta - secret 2k13 (fallen skies edit) - mar 30 2013  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvS_8ioLZ6jB"
      },
      "source": [
        "<a id=\"Remove_HTML_tags\"></a>\n",
        "\n",
        "### Remove HTML tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:39.833807Z",
          "iopub.status.busy": "2021-11-13T10:20:39.833453Z",
          "iopub.status.idle": "2021-11-13T10:20:39.839093Z",
          "shell.execute_reply": "2021-11-13T10:20:39.837859Z",
          "shell.execute_reply.started": "2021-11-13T10:20:39.833777Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4xZO_wcZ6jB",
        "outputId": "71a783e5-c511-495d-8ddf-5d6075a269a9"
      },
      "source": [
        "#complete the function below to remove html tags\n",
        "\n",
        "\n",
        "def remove_html(text):\n",
        "    \"\"\"\n",
        "        Remove the html in sample text\n",
        "    \"\"\"\n",
        "    import re\n",
        "    text = re.sub('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '',str(text) )\n",
        "    return text\n",
        "\n",
        "# remove html from the text\n",
        "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_html(x))\n",
        "\n",
        "# double check\n",
        "print(train_df[\"text\"][62])\n",
        "print(train_df[\"text_clean\"][62])\n",
        "print(train_df[\"text\"][7385])\n",
        "print(train_df[\"text_clean\"][7385])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rene Ablaze &amp; Jacinta - Secret 2k13 (Fallen Skies Edit) - Mar 30 2013  https://t.co/7MLMsUzV1Z\n",
            "rene ablaze  jacinta - secret 2k13 (fallen skies edit) - mar 30 2013  \n",
            "NW Michigan #WindStorm (Sheer) Recovery Updates: Leelanau &amp; Grand Traverse - State of Emergency 2b extended http://t.co/OSKfyj8CK7 #BeSafe\n",
            "nw michigan #windstorm (sheer) recovery updates: leelanau  grand traverse - state of emergency 2b extended  #besafe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uah1R_qxZ6jC"
      },
      "source": [
        "<a id=\"Remove_Non_ASCII\"></a>\n",
        "\n",
        "### Remove Non-ASCI:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:39.909211Z",
          "iopub.status.busy": "2021-11-13T10:20:39.908999Z",
          "iopub.status.idle": "2021-11-13T10:20:39.913459Z",
          "shell.execute_reply": "2021-11-13T10:20:39.912481Z",
          "shell.execute_reply.started": "2021-11-13T10:20:39.909188Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RoeACNGZ6jC",
        "outputId": "014c0519-ed89-4b62-99e9-7f7b9d840c38"
      },
      "source": [
        "#complete the function below to remove non-ascii characters\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    \"\"\"\n",
        "        Remove non-ASCII characters \n",
        "    \"\"\"\n",
        "    import re\n",
        "    text = re.sub(r'[^\\x00-\\x7F]', '',str(text) )\n",
        "    return text\n",
        "\n",
        "# removing non-ascii characters from the text\n",
        "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))\n",
        "\n",
        "# double check\n",
        "print(train_df[\"text\"][38])\n",
        "print(train_df[\"text_clean\"][38])\n",
        "print(train_df[\"text\"][7586])\n",
        "print(train_df[\"text_clean\"][7586])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barbados #Bridgetown JAMAICA ÛÒ Two cars set ablaze: SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintende...  http://t.co/wDUEaj8Q4J\n",
            "barbados #bridgetown jamaica  two cars set ablaze: santa cruz  head of the st elizabeth police superintende...  \n",
            "#Sismo DETECTADO #JapÌ_n 15:41:07 Seismic intensity 0 Iwate Miyagi JST #?? http://t.co/gMoUl9zQ2Q\n",
            "#sismo detectado #jap_n 15:41:07 seismic intensity 0 iwate miyagi jst #?? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9uMET0JZ6jC"
      },
      "source": [
        "<a id=\"Remove_punctuations\"></a>\n",
        "\n",
        "## Remove punctuations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:39.953064Z",
          "iopub.status.busy": "2021-11-13T10:20:39.952769Z",
          "iopub.status.idle": "2021-11-13T10:20:39.958153Z",
          "shell.execute_reply": "2021-11-13T10:20:39.957085Z",
          "shell.execute_reply.started": "2021-11-13T10:20:39.953040Z"
        },
        "scrolled": true,
        "id": "7T8Q0IjlZ6jC"
      },
      "source": [
        "#complete the function below to remove punctuations\n",
        "\n",
        "def remove_punct(text):\n",
        "    \"\"\"\n",
        "        Remove the punctuation\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# removing punctuations from the text\n",
        "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_punct(x))\n",
        "\n",
        "# double check\n",
        "print(train_df[\"text\"][5])\n",
        "print(train_df[\"text_clean\"][5])\n",
        "print(train_df[\"text\"][7597])\n",
        "print(train_df[\"text_clean\"][7597])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9wdh7TaZ6jC"
      },
      "source": [
        "<a id=\"Text_Preprocessing\"></a>\n",
        "\n",
        "# Text Preprocessing:\n",
        "\n",
        "<a id=\"Tokenization\"></a>\n",
        "## Tokenization\n",
        "Tokenization is a common technique that split a sentence into tokens, where a token could be characters, words, phrases, symbols, or other meaningful elements. By breaking sentences into smaller chunks, that would help to investigate the words in a sentence and also the subsequent steps in the NLP pipeline, such as stemming. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:40.160184Z",
          "iopub.status.busy": "2021-11-13T10:20:40.159958Z",
          "iopub.status.idle": "2021-11-13T10:20:41.111234Z",
          "shell.execute_reply": "2021-11-13T10:20:41.109647Z",
          "shell.execute_reply.started": "2021-11-13T10:20:40.160161Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "RNU2NhzeZ6jD",
        "outputId": "955874cb-a9ba-444f-92a9-af69b2ba3a83"
      },
      "source": [
        "# Tokenizing the tweet base texts and save it in a column called \"tokenized\"\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "train_df['tokenized'] = train_df['text_clean'].apply(word_tokenize)\n",
        "train_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>our deeds are the reason of this #earthquake m...</td>\n",
              "      <td>[our, deeds, are, the, reason, of, this, #, ea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>forest fire near la ronge sask. canada</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>all residents asked to 'shelter in place' are ...</td>\n",
              "      <td>[all, residents, asked, to, 'shelter, in, plac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
              "      <td>[just, got, sent, this, photo, from, ruby, #, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                          tokenized\n",
              "0   1  ...  [our, deeds, are, the, reason, of, this, #, ea...\n",
              "1   4  ...   [forest, fire, near, la, ronge, sask, ., canada]\n",
              "2   5  ...  [all, residents, asked, to, 'shelter, in, plac...\n",
              "3   6  ...  [13,000, people, receive, #, wildfires, evacua...\n",
              "4   7  ...  [just, got, sent, this, photo, from, ruby, #, ...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuZ0ZJpZZ6jD"
      },
      "source": [
        "<a id=\"Remove_Stop_Words\"></a>\n",
        "\n",
        "## Remove Stop Words (or/and Frequent words/ Rare words):\n",
        "Stop words are common words in any language that occur with a high frequency but do not deliver meaningful information for the whole sentence. For example, {“a”, “about”, “above”, “across”, “after”, “afterward”, “again”, ...} can be considered as stop words. Traditionally, we could remove all of them in the text preprocessing stage. However, refer to the example from the [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action) book: \n",
        "> * Mark reported to the CEO\n",
        "> * Suzanne reported as the CEO to the board \n",
        "\n",
        "> In your NLP pipeline, you might create 4-grams such as reported to the CEO and reported as the CEO. If you remove the stop words from the 4-grams, both examples would be reduced to \"reported CEO\", and you would lack the information about the professional hierarchy. In the first example, Mark could have been an assistant to the CEO, whereas in the second example Suzanne was the CEO reporting to the board. Unfortunately, retaining the stop words within your pipeline creates another problem: it increases the length of the n-grams required to make use of these connections formed by the otherwise meaningless stop words. This issue forces us to retain at least 4-grams if you want to avoid the ambiguity of the human resources example.\n",
        "> Designing a filter for stop words depends on your particular application.\n",
        "\n",
        "In short, removing stop words is a common method in NLP text preprocessing, whereas, it needs to be experimented carefully depending on different situations. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:41.113323Z",
          "iopub.status.busy": "2021-11-13T10:20:41.113005Z",
          "iopub.status.idle": "2021-11-13T10:20:41.155762Z",
          "shell.execute_reply": "2021-11-13T10:20:41.155096Z",
          "shell.execute_reply.started": "2021-11-13T10:20:41.113297Z"
        },
        "id": "RzAP6H3DZ6jD"
      },
      "source": [
        "# Remove stopwords from train_df['tokenized'] and save it in another column called 'stopwords_removed'\n",
        "# Hint: Use pandas apply function with lambda function (the way we did in previous cells) and you can use list\n",
        "# comprehension to remove stop words\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "#complete code below\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEao1NuIZ6jD"
      },
      "source": [
        "<a id=\"Stemming\"></a>\n",
        "\n",
        "## Stemming\n",
        "Stemming is a process of extracting a root word - identifying a common stem among various forms (e.g., singular and plural noun form) of a word, for example, the words \"gardening\", \"gardener\" or \"gardens\" share the same stem, garden. Stemming uproots suffixes from words to merge words with similar meanings under their standard stem.\n",
        "\n",
        "There are three major stemming algorithms in use nowadays:\n",
        "- **Porter** - PorterStemmer()): This stemming algorithm is an older one. It’s from the 1980s and its main concern is removing the common endings to words so that they can be resolved to a common form. It’s not too complex and development on it is frozen. Typically, it’s a nice starting basic stemmer, but it’s not really advised to use it for any production/complex application. Instead, it has its place in research as a nice, basic stemming algorithm that can guarantee reproducibility. It also is a very gentle stemming algorithm when compared to others.\n",
        "\n",
        "- **Snowball** -  SnowballStemmer(): This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer. That being said, it is also more aggressive than the Porter stemmer. A lot of the things added to the Snowball stemmer were because of issues noticed with the Porter stemmer. There is about a 5% difference in the way that Snowball stems versus Porter.\n",
        "\n",
        "- **Lancaster** -LancasterStemmer(): Just for fun, the Lancaster stemming algorithm is another algorithm that you can use. This one is the most aggressive stemming algorithm of the bunch. However, if you use the stemmer in NLTK, you can add your own custom rules to this algorithm very easily. It’s a good choice for that. One complaint around this stemming algorithm though is that it sometimes is overly aggressive and can really transform words into strange stems. Just make sure it does what you want it to before you go with this option!\n",
        "\n",
        "source: http://hunterheidenreich.com/blog/stemming-lemmatization-what/\n",
        "\n",
        "### PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:41.157015Z",
          "iopub.status.busy": "2021-11-13T10:20:41.156792Z",
          "iopub.status.idle": "2021-11-13T10:20:41.162619Z",
          "shell.execute_reply": "2021-11-13T10:20:41.161640Z",
          "shell.execute_reply.started": "2021-11-13T10:20:41.156991Z"
        },
        "scrolled": true,
        "id": "y2JM7PhAZ6jE"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def porter_stemmer(text):\n",
        "    \"\"\"\n",
        "        Stem words in list of tokenized words with PorterStemmer\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "\n",
        "train_df['porter_stemmer'] = train_df['stopwords_removed'].apply(lambda x: porter_stemmer(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8as5WHkTZ6jE"
      },
      "source": [
        "<a id=\"Lemmatization\"></a>\n",
        "\n",
        "## Lemmatization:\n",
        "According to the [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf) book:\n",
        "> Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. Lemmatizing each of these forms to the same lemma will let us ﬁnd all mentions of words in Russian like Moscow. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective story.\n",
        "\n",
        "and the book [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action):\n",
        "> Some lemmatizers use the word’s part of speech (POS) tag in addition to its spelling to help improve accuracy. The POS tag for a word indicates its role in the grammar of a phrase or sentence. For example, the noun POS is for words that refer to “people, places, or things” within a phrase. An adjective POS is for a word that modifies or describes a noun. A verb refers to an action. The POS of a word in isolation cannot be determined. The context of a word must be known for its POS to be identified. So some advanced lemmatizers can’t be run-on words in isolation.\n",
        "\n",
        "For example, the \"good\", \"better\" or \"best\" is lemmatized into good and the verb \"gardening\" should be lemmatized to \"to garden\", while the \"garden\" and \"gardener\" are both different lemmas. In this notebook, we will also explore on both lemmatize on without POS-Tagging and POS-Tagging examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "any1MPmSZ6jE"
      },
      "source": [
        "<a id=\"Lemmatization\"></a>\n",
        "\n",
        "### Lemmatization:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:48.176982Z",
          "iopub.status.busy": "2021-11-13T10:20:48.176700Z",
          "iopub.status.idle": "2021-11-13T10:20:48.785702Z",
          "shell.execute_reply": "2021-11-13T10:20:48.784673Z",
          "shell.execute_reply.started": "2021-11-13T10:20:48.176953Z"
        },
        "id": "5WqZ5yiGZ6jE"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#Apply lemmatization on train_df['stopwords_removed'] and save it in train_df['lemmatize_word']\n",
        "\n",
        "\n",
        "# Next, remove any newly formed stop words in train_df['lemmatize_word']\n",
        "\n",
        "\n",
        "\n",
        "#Joining the lemmatized tokens back to text\n",
        "train_df['lemmatize_text'] = [' '.join(map(str, l)) for l in train_df['lemmatize_word']] \n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:49.466150Z",
          "iopub.status.busy": "2021-11-13T10:20:49.465755Z",
          "iopub.status.idle": "2021-11-13T10:20:49.473793Z",
          "shell.execute_reply": "2021-11-13T10:20:49.472384Z",
          "shell.execute_reply.started": "2021-11-13T10:20:49.466110Z"
        },
        "id": "YoS_Dq2WZ6jE"
      },
      "source": [
        "#comparison between original text and lemmatized text\n",
        "\n",
        "print(train_df[\"text\"][8])\n",
        "print(train_df[\"lemmatize_word\"][8])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hveRg8NZ6jE"
      },
      "source": [
        "Comparison between original text and the lammatized text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:49.475416Z",
          "iopub.status.busy": "2021-11-13T10:20:49.475133Z",
          "iopub.status.idle": "2021-11-13T10:20:49.508126Z",
          "shell.execute_reply": "2021-11-13T10:20:49.507045Z",
          "shell.execute_reply.started": "2021-11-13T10:20:49.475386Z"
        },
        "id": "VQhFCa26Z6jE"
      },
      "source": [
        "display(train_df[\"text\"][0], train_df[\"lemmatize_text\"][0])\n",
        "display(train_df[\"text\"][5], train_df[\"lemmatize_text\"][5])\n",
        "display(train_df[\"text\"][10], train_df[\"lemmatize_text\"][10])\n",
        "display(train_df[\"text\"][15], train_df[\"lemmatize_text\"][15])\n",
        "display(train_df[\"text\"][20], train_df[\"lemmatize_text\"][20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS2iKywiZ6jG"
      },
      "source": [
        "# Text Features Extraction:\n",
        "\n",
        "## Weighted Words - Bag of Words (BoW) - Bag of n-grams:\n",
        "**Watch this video: https://www.youtube.com/watch?v=IKgBLTeQQL8**\n",
        "\n",
        "* N-gram is a sequence that contains n-elements (characters, words, etc). A single word such a \"apple\", \"orange\" is a Uni-gram; hence, \"red apple\" \"big orange\" is bi-gram and \"red ripped apple\", \"big orange bag\" is tri-gram. \n",
        "* Bags of words: Vectors of word counts or frequencies \n",
        "* Bags of n-grams: Counts of word pairs (bigrams), triplets (trigrams), and so on\n",
        "\n",
        "> The bag-of-words/ bag-of-n-gram model is a reduced and simpliﬁed representation of a text document from selected parts of the text, based on speciﬁc criteria, such as word frequency.\n",
        "> \n",
        "> In a BoW, a body of text, such as a document or a sentence, is thought of like a bag of words. Lists of words are created in the BoW process. These words in a matrix are not sentences which structure sentences and grammar, and the semantic relationship between these words are ignored in their collection and construction. The words are often representative of the content of a sentence. While grammar and order of appearance are ignored, multiplicity is counted and may be used later to determine the focus points of the documents.\n",
        "> \n",
        "> Example:\n",
        "> Document\n",
        "> \n",
        "> “As the home to UVA’s recognized undergraduate and graduate degree programs in systems engineering. In the UVA Department of Systems and Information Engineering, our students are exposed to a wide range of range”\n",
        "> \n",
        "> Bag-of-Words (BoW):\n",
        "> {“As”, “the”, “home”, “to”, “UVA’s”, “recognized”, “undergraduate”, “and”, “graduate”, “degree”, “program”, “in”, “systems”, “engineering”, “in”, “Department”, “Information”,“students”, “ ”,“are”, “exposed”, “wide”, “range” }\n",
        "> \n",
        "> Bag-of-Feature (BoF)\n",
        "> Feature = {1,1,1,3,2,1,2,1,2,3,1,1,1,2,1,1,1,1,1,1}\n",
        "\n",
        "### Frequency Vectors - CountVectorizer:\n",
        "We will implement the Bag of Words/ Bag of n-grams text representation via sklearn - CountVectorizer function.\n",
        "The code will test with a sample corpus of the first five sentence of the dataset, then print out the output of uni-gram, bi-gram and tri-gram. Finaly, we also run on the whole dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:49.510031Z",
          "iopub.status.busy": "2021-11-13T10:20:49.509756Z",
          "iopub.status.idle": "2021-11-13T10:20:49.517122Z",
          "shell.execute_reply": "2021-11-13T10:20:49.516090Z",
          "shell.execute_reply.started": "2021-11-13T10:20:49.510003Z"
        },
        "scrolled": true,
        "id": "XjvSlYWaZ6jG"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# implement CountVectorizer for 1-gram, 2-gram and 3-gram on train_df[\"lemmatize_text\"].tolist()\n",
        "\n",
        "def cv(data, ngram = 1, MAX_NB_WORDS = 75000):\n",
        "    \"\"\"\n",
        "        return transformed_dataset, CountVectorizer_object\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "# applying 1-gram, 2-gram and 3-gram into the whole dataset\n",
        "\n",
        "train_df_corpus = train_df[\"lemmatize_text\"].tolist()\n",
        "train_df_em_1gram, cv_1gram = cv(train_df_corpus, ngram=1)\n",
        "train_df_em_2gram, cv_2gram = cv(train_df_corpus, ngram=2)\n",
        "train_df_em_3gram, cv_3gram = cv(train_df_corpus, ngram=3)\n",
        "\n",
        "print(len(train_df_corpus))\n",
        "print(train_df_em_1gram.shape)\n",
        "print(train_df_em_2gram.shape)\n",
        "print(train_df_em_3gram.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RZeZghlZ6jG"
      },
      "source": [
        "<a id=\"TF_IDF\"></a>\n",
        "\n",
        "### Term Frequency-Inverse Document Frequency (TF-IDF):\n",
        "**Watch this video: https://www.youtube.com/watch?v=D2V1okCEsiE**\n",
        "> The Inverse Document Frequency (IDF) as a method to be used in conjunction with term frequency in order to lessen the effect of implicitly common words in the corpus. IDF assigns a higher weight to words with either high or low frequencies term in the document. This combination of TF and IDF is well known as Term Frequency-Inverse document frequency (TF-IDF). The mathematical representation of the weight of a term in a document by TF-IDF is given in Equation: \n",
        "> $$ W(d,t) = TF(d,t) * log \\frac{N}{df(t)}$$\n",
        "> Here N is the number of documents and $df(t)$ is the number of documents containing the term t in the corpus. The ﬁrst term in the equation improves the recall while the second term improves the precision of the word embedding. Although TF-IDF tries to overcome the problem of common terms in the document, it still suffers from some other descriptive limitations. Namely, TF-IDF cannot account for the similarity between the words in the document since each word is independently presented as an index. However, with the development of more complex models in recent years, new methods, such as word embedding, have been presented that can incorporate concepts such as similarity of words and part of speech tagging.\n",
        "\n",
        "(source: [Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067))\n",
        "\n",
        "We also implement the TF-IDF via sklearn TfidfVectorizer function, the experiments are similar to the previous section\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:52.328871Z",
          "iopub.status.busy": "2021-11-13T10:20:52.328590Z",
          "iopub.status.idle": "2021-11-13T10:20:52.345690Z",
          "shell.execute_reply": "2021-11-13T10:20:52.343526Z",
          "shell.execute_reply.started": "2021-11-13T10:20:52.328842Z"
        },
        "scrolled": true,
        "id": "tao15_t8Z6jH"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#implement tfid vectorizer for ngram=1, on train_df[\"lemmatize_text\"].tolist() and print its shape\n",
        "def TFIDF(data, ngram = 1, MAX_NB_WORDS = 75000):\n",
        "    \"\"\"\n",
        "        return transformed_dataset, TfidfVectorizer_object\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "train_df_corpus = train_df[\"lemmatize_text\"].tolist()\n",
        "train_df_tfidf_1gram, tfidf_1gram = TFIDF(train_df_corpus, 1)\n",
        "\n",
        "print(len(train_df_corpus))\n",
        "print(train_df_tfidf_1gram.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaN-ofbhZ6jH"
      },
      "source": [
        "<a id=\"Word_Embedding\"></a>\n",
        "\n",
        "## Word Embedding:\n",
        "\n",
        "> **Word vectors** are numerical vector representations of word semantics, or meaning, including literal and implied meaning. So word vectors can capture the connotation of words, like “peopleness,” “animalness,” “placeness,” “thingness,” and even “conceptness.” And they combine all that into a dense vector (no zeros) of floating point values. This dense vector enables queries and logical reasoning.\n",
        "\n",
        "(source: [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action))\n",
        "\n",
        "> Even though we have syntactic word representations, it does not mean that the model captures the semantics meaning of the words. On the other hand, bag-of-word models do not respect the semantics of the word. For example, words “airplane”, “aeroplane”, “plane”, and “aircraft” are often used in the same context. However, the vectors corresponding to these words are orthogonal in the bag-of-words model. This issue presents a serious problem to understanding sentences within the model. The other problem in the bag-of-word is that the order of words in the phrase is not respected. The n-gram does not solve this problem so a similarity needs to be found for each word in the sentence. Many researchers worked on word embedding to solve this problem. The Word2Vec propose a simple single-layer architecture based on the inner product between two word vectors.\n",
        "\n",
        "> Word embedding is a feature learning technique in which each word or phrase from the vocabulary is mapped to a N dimension vector of real numbers. Various word embedding methods have been proposed to translate unigrams into understandable input for machine learning algorithms. This work focuses on Word2Vec, GloVe, and FastText, three of the most common methods that have been successfully used for deep learning techniques.\n",
        "\n",
        "(source: [Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067))\n",
        "\n",
        "<a id=\"Basic_Word_Embedding\"></a>\n",
        "### Basic Word Embedding Methods:\n",
        "\n",
        "**Watch this video: https://www.youtube.com/watch?v=pO_6Jk0QtKw**\n",
        "#### Word2Vec:\n",
        "\n",
        "[T. Mikolov et al.](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) presented the Word2vec in 2013, which learns the meaning of words merely by processing a large corpus of unlabeled text. The Word2Vec approach uses shallow neural networks with two hidden layers, continuous bag-of-words (CBOW), and the Skip-gram model to create a high dimension vector for each word. This unsupervised nature of Word2vec is what makes it so powerful. The world is full of unlabeled, uncategorized, unstructured natural language text.\n",
        "\n",
        "We will implement the Word2vec via gensim libary \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:54.904469Z",
          "iopub.status.busy": "2021-11-13T10:20:54.904142Z",
          "iopub.status.idle": "2021-11-13T10:20:57.389866Z",
          "shell.execute_reply": "2021-11-13T10:20:57.389040Z",
          "shell.execute_reply.started": "2021-11-13T10:20:54.904438Z"
        },
        "scrolled": true,
        "id": "BosEiDzTZ6jH"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# train the Word2vec model on train_df[\"lemmatize_word\"].tolist()\n",
        "\n",
        "# print summary of the model\n",
        "\n",
        "# print vector for any one word\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:57.391319Z",
          "iopub.status.busy": "2021-11-13T10:20:57.391049Z",
          "iopub.status.idle": "2021-11-13T10:20:57.396403Z",
          "shell.execute_reply": "2021-11-13T10:20:57.395837Z",
          "shell.execute_reply.started": "2021-11-13T10:20:57.391291Z"
        },
        "id": "Bf1acxmSZ6jH"
      },
      "source": [
        "#print the similarity between any two words in the dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnTZAyF3Z6jH"
      },
      "source": [
        "<a id=\"GloVe\"></a>\n",
        "\n",
        "#### Global Vectors for Word Representation (GloVe):\n",
        "> Another powerful word embedding technique that has been used for text classiﬁcation is [Global Vectors (GloVe)](https://nlp.stanford.edu/pubs/glove.pdf). The approach is very similar to the Word2Vec method, where each word is presented by a high dimension vector and trained based on the surrounding words over a huge corpus. The pre-trained word embedding used in many works is based on 400,000 vocabularies trained over Wikipedia 2014 and Gigaword 5 as the corpus and 50 dimensions for word presentation. GloVe also provides other pre-trained word vectorizations with 100, 200, 300 dimensions which are trained over even bigger corpora, including Twitter content.\n",
        "\n",
        "We will create our GloVe's sentence embeddings  via gensim libary with the pre-trained word vectors on the dataset from Wikipedia 2014 + Gigaword 5 (source: https://github.com/stanfordnlp/GloVe) and see the embedding output on the sample sentence from the our dataset. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:20:58.560808Z",
          "iopub.status.busy": "2021-11-13T10:20:58.560428Z",
          "iopub.status.idle": "2021-11-13T10:22:43.013605Z",
          "shell.execute_reply": "2021-11-13T10:22:43.012543Z",
          "shell.execute_reply.started": "2021-11-13T10:20:58.560780Z"
        },
        "scrolled": true,
        "id": "pg1H50LDZ6jH"
      },
      "source": [
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import gensim\n",
        "#download glove embeddings from: https://www.kaggle.com/anindya2906/glove6b and save it in the same folder as this task\n",
        "\n",
        "glove_input_file = \"glove.6B.100d.txt\" #if you have named the file differently, then make changes here\n",
        "word2vec_output_file = \"glove.6B.100d.txt.word2vec\"\n",
        "glove2word2vec(glove_input_file, word2vec_output_file) #converting glove_input_file in word format to word2vec format\n",
        "\n",
        "glove_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False, limit=200000) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJtj4McsZ6jI"
      },
      "source": [
        "Compare the similarity between \"cat\" vs. \"kitten\" and \"cat\" vs. \"cats\" from GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-13T10:22:43.014821Z",
          "iopub.status.busy": "2021-11-13T10:22:43.014607Z",
          "iopub.status.idle": "2021-11-13T10:22:43.022062Z",
          "shell.execute_reply": "2021-11-13T10:22:43.021216Z",
          "shell.execute_reply.started": "2021-11-13T10:22:43.014799Z"
        },
        "id": "YOaAQmytZ6jI"
      },
      "source": [
        "# Compare the similarity between \"cat\" vs. \"kitten\" and \"cat\" vs. \"cats\" from the GloVe Model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4JYuP7kZ6jI"
      },
      "source": [
        "## T-SNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRryuNFMZ6jI"
      },
      "source": [
        "**Watch This Video: https://www.youtube.com/watch?v=NEaUSP4YerM**<br>\n",
        "Now, lets visualize some of our embeddings. To plot embeddings with a dimensionality of 100 or more, we first need to map them to a dimensionality of 2. We do this with the popular t-SNE method. T-SNE, short for t-distributed Stochastic Neighbor Embedding, helps us visualize high-dimensional data by mapping similar data to nearby points and dissimilar data to distance points in the low-dimensional space.\n",
        "\n",
        "T-SNE is present in Scikit-learn. To run it, we just have to specify the number of dimensions we'd like to map the data to (n_components), and the similarity metric that t-SNE should use to compute the similarity between two data points (metric). We're going to map to 2 dimensions and use the cosine as our similarity metric. Additionally, we use PCA as an initialization method to remove some noise and speed up computation. The Scikit-learn user guide contains some additional tips for optimizing performance.\n",
        "\n",
        "Plotting all the embeddings in our vector space would result in a very crowded figure where the labels are hardly legible. Therefore we'll focus on a subset of embeddings by selecting the 200 most similar words to a target word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqwtKSCBZ6jI"
      },
      "source": [
        "#tSNE\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "target_word = \"body\"\n",
        "selected_words = [w[0] for w in model.wv.most_similar(positive=[target_word], topn=200)] + [target_word]\n",
        "embeddings = [model.wv[w] for w in selected_words] + model.wv[\"body\"]\n",
        "\n",
        "mapped_embeddings = TSNE(n_components=2, metric='cosine', init='pca').fit_transform(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5mLbga7Z6jI"
      },
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "x = mapped_embeddings[:,0]\n",
        "y = mapped_embeddings[:,1]\n",
        "plt.scatter(x, y)\n",
        "\n",
        "for i, txt in enumerate(selected_words):\n",
        "    plt.annotate(txt, (x[i], y[i]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}